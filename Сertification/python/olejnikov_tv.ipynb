{
  "metadata": {
    "name": "olejnikov_tv",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession.builder\\\n        .master(\"local[*]\")\\\n        .appName(\u0027olejnikov_tv\u0027)\\\n        .enableHiveSupport()\\\n        .getOrCreate()\n\ndf \u003d spark.sql(\"select * from olejnikov.tv_dataset_v\")\n\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ntype(df)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf.createOrReplaceTempView(\"tv_tab\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf1 \u003d spark.sql(\"\\\n    select channel_name, round((sum(UNIX_TIMESTAMP(time_end) - UNIX_TIMESTAMP(time_start))) / 3600) as sum_time, count(channel_name) as count_views \\\n    from tv_tab \\\n    group by channel_name \\\n    order by sum_time desc \\\n\").toPandas()\n\ndf1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf2 \u003d spark.sql(\"\\\n    select to_date(time_start) as time_dates, round((sum(UNIX_TIMESTAMP(time_end) - UNIX_TIMESTAMP(time_start))) / 3600) as sum_time, count(*) as count_views \\\n    from tv_tab \\\n    group by to_date(time_start) \\\n    order by to_date(time_start) \\\n\").toPandas()\n\ndf2"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport matplotlib.pyplot as plt  \nimport pandas as pd \nimport numpy as np\n\nfrom pyspark.sql import functions as f"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf2.plot(x\u003d\u0027time_dates\u0027, y\u003d[\u0027sum_time\u0027, \u0027count_views\u0027], kind\u003d\u0027bar\u0027)\nplt.ylabel(\u0027Общее время (часы) / количество просмотров\u0027)\nplt.xlabel(\"Дата\")\nplt.title(\"Количество просмотров по дням\")\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nhours \u003d df1[\u0027sum_time\u0027].head(10)\nchannels \u003d df1[\u0027channel_name\u0027].head(10)\nfig, ax \u003d plt.subplots()\nax.pie(hours, labels\u003dchannels, autopct\u003d\u0027%1.2f%%\u0027, wedgeprops\u003ddict(width\u003d0.7))\nplt.title(\"The most viewered channels\")\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nhours \u003d df1[\u0027sum_time\u0027].head(6)\nhours.loc[len(hours.index)] \u003d df1[\u0027sum_time\u0027].tail(138).sum()\nchannels \u003d df1[\u0027channel_name\u0027].head(6)\nchannels.loc[len(channels.index)] \u003d \u0027Прочие\u0027\nmyexplode \u003d [0, 0, 0, 0, 0, 0, 0.1,]\nfig, ax \u003d plt.subplots()\nax.pie(hours, labels\u003dchannels, autopct\u003d\u0027%1.2f%%\u0027, explode \u003d myexplode, shadow \u003d True)\nplt.title(\"Топ просматриваемых каналов\")\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf3 \u003d df1\ndf3[\u0027rating\u0027] \u003d round(df3[\u0027sum_time\u0027] / df3[\u0027count_views\u0027], 2)\ndf4 \u003d df3.sort_values(by \u003d \u0027rating\u0027, ascending\u003dFalse)\n\ndf4.tail(15)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ny \u003d df4[\u0027channel_name\u0027].tail(10)\nx \u003d df4[\u0027rating\u0027].tail(10)\nplt.barh(y, x, color\u003d\u0027green\u0027)\nplt.ylabel(\u0027Каналы\u0027)\nplt.xlabel(\"Рейтинг\")\nplt.title(\"Наименее просматриваемые каналы \\n(общее количество часов просмотра / количество просмотров)\")\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nz.show(df4.head(20))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}